{
  "url": "https://ladiaria.com.uy/ciencia/articulo/2024/10/el-nobel-de-fisica-de-2024-nos-recuerda-que-la-inteligencia-artificial-de-artificial-no-tiene-nada/",
  "title": "El Nobel de Física de 2024 nos recuerda que la Inteligencia Artificial de artificial no tiene nada",
  "description": "El premio dado a John Hopfield y Geoffrey Hinton por sus aportes “que permiten el aprendizaje automático con redes neuronales artificiales” deja en evidencia que aún las más intrincadas tecnologías son producto de la curiosidad humana y del trabajo incansable de grandes colectivos durante décadas.",
  "links": [
    "https://ladiaria.com.uy/ciencia/articulo/2024/10/el-nobel-de-fisica-de-2024-nos-recuerda-que-la-inteligencia-artificial-de-artificial-no-tiene-nada/",
    "https://ladiaria.com.uy/ciencia/articulo/2024/10/el-nobel-de-fisica-de-2024-nos-recuerda-que-la-inteligencia-artificial-de-artificial-no-tiene-nada/?display=amp"
  ],
  "image": "https://ladiaria.com.uy/media/photologue/photos/cache/AFP__20241008__36JL7X8__v1__HighRes__SwedenNobelPrizePhysics_copy_article_main.jpg?r=ANKflw5df4DqTtnM",
  "content": "<div>\n    <p>“Los métodos y conceptos pioneros desarrollados por Hopfield y Hinton han sido fundamentales para dar forma al campo de las redes neuronales artificiales. Además, Hinton desempeñó un papel destacado en los esfuerzos por extender los métodos a las redes neuronales artificiales profundas y densas”, dice el Comité del Nobel de Física de la Real Academia Sueca de Ciencias en el documento que detalla los antecedentes científicos para dar, en forma compartida, el premio al físico teórico John Hopfield, y al a veces llamado “padrino de la Inteligencia Artificial”, Geoffrey Hinton.</p>\n<p>El premio, dado por el abordaje desde la física -sí, con muchas ecuaciones y parámetros- en la concepción y funcionamiento de modelos de redes neuronales que se inspiraban en lo que sucede en nuestros cerebros, refiere a aportes que tuvieron lugar, fundamentalmente, durante la década de 1980 y principios de 1990.</p>\n<p>“Las computadoras digitales modernas son recién llegadas al mundo de la computación. Las computadoras biológicas (el cerebro y el sistema nervioso de los animales y los seres humanos) existen desde hace millones de años y son extraordinariamente eficaces para procesar información sensorial y controlar las interacciones de los animales con su entorno. Tareas como alcanzar un sándwich, reconocer una cara o recordar cosas asociadas con el sabor de las magdalenas son cálculos tanto como lo son la multiplicación y la ejecución de videojuegos”, escribían David Tack y el hoy laureado con medio Nobel de Física John Hopfield en 1987 en un artículo de la revista <em>Scientific American</em>. “El hecho de que la computación biológica sea tan eficaz sugiere que es posible lograr capacidades similares en dispositivos artificiales basados en los principios de diseño de los sistemas neuronales”, agregaban, adelantando que habían estudiado “una serie de circuitos electrónicos de 'redes neuronales' que pueden llevar a cabo cálculos importantes”. </p>\n<p>“Estos modelos simples tienen sólo un parecido metafórico con los ordenadores de la naturaleza, pero ofrecen una forma elegante y diferente de pensar en la computación de las máquinas, que está inspirando nuevos diseños de chips y ordenadores microelectrónicos. También pueden proporcionar nuevos conocimientos sobre los sistemas biológicos”, decían entonces llenos de esperanza.</p>\n<p>Que las personas que hacen ciencia que logran cosas fantásticas se paran sobre hombros de gigantes debe ser una de las frases más repetidas de la comunicación científica. Que la ciencia es una actividad colectiva debe ser la segunda. Sin embargo, el premio Nobel hace caso omiso a todo esto y decide premiar a personas -salvo alguna excepción-. Pero como ya dejaban ver Tank y Hopfield en 1987, la cosa venía de antes: “La investigación actual sobre este tema se basa en una larga historia de esfuerzos por plasmar los principios de la computación biológica en modelos matemáticos. El esfuerzo comenzó con las investigaciones pioneras de las neuronas como dispositivos lógicos realizadas por Warren McCulloch y Walter Pitts en 1943. En la década de 1960, Frank Rosenblatt, de la Universidad de Cornell, y Bernard Widrow, que ahora está en la Universidad de Stanford, crearon 'neuronas adaptativas' y redes simples que aprenden. Adaline (abreviatura de elemento lineal adaptativo) de Widrow es un sistema de una sola neurona que puede aprender a reconocer un patrón, como una letra, independientemente de su orientación o tamaño. Durante los años 1960 y 1970, un pequeño número de investigadores, como Shunichi Amari, Leon 'Cooper, Kunihiko Fukushima y Stephen Grossberg, intentaron modelar más de cerca el comportamiento de las neuronas reales en redes computacionales y desarrollar matemáticas y arquitecturas para extraer características de patrones, clasificar patrones y crear 'memoria asociativa', en la que fragmentos de la información almacenada sirven para recuperar una memoria completa”, sostenían. Sobre todo esto se pararon y agregaron su esfuerzo intelectual: “nuestro trabajo se ha centrado en los principios que dan lugar al comportamiento computacional en un tipo particular de circuito similar a los de las neuronas”.</p>\n<p>Fue entonces, durante la década de 1980, que el trabajo de Hopefield -¡y colegas!- llevó a, como dice el Comité de Física del Nobel, implicó “una contribución fundamental a nuestra comprensión de las capacidades computacionales de las redes neuronales”. Sobre estos hombros se paró entonces Geoffrey Hinton.</p>\n<h2>El padrino de la IA</h2>\n<p>Geoffrey Hinton, nacido en Reino Unido, se dedicó casi desde sus inicios a la inteligencia artificial. Como dice en su artículo de 1985 titulado “Aprendiendo en redes paralelas”, las computadoras de entonces podían realizar cálculos con números con gran cantidad de cifras, pero eran “notablemente malas al hacer lo que cualquier niño de cinco años podría hacer”. Decía entonces, que una posible explicación para ello podría ser que “no programamos a las computadoras de la forma adecuada”. Otra posible explicación, decía, podría pasar por el simple hecho de que “los cerebros y las computadoras trabajan de forma diferente”, y reconociendo que “cualquier estilo de computación podría ser <em>simulado</em> por una computadora digital” -la itálica es del propio Hinton-, podría pasar que “cuando un tipo de maquina simula ser otra totalmente distinta, puede ser muy lenta”. Ponía entonces un ejemplo brillante: “simular todas las neuronas en el cerebro humano en tempo real llevaría miles de computadoras enormes”, mientras que “simular todas las operaciones aritméticas que ocurren en una Cray implicaría miles de millones de personas” -Cray era una línea de supercomputadoras de la época producidas por la Cray Computer Corporation, y aún hoy siguen estando, aunque fue comprada por distintas empresas-.</p>\n<p>Junto con colegas como Terrence Sejnowski -sí, casi nadie trabaja solo señores del Nobel- Hinton entonces llevó el modelo de redes neuronales Hopfield más allá, en lo que se conoce como la Maquina de Boltzmann, que tiene un abordaje probabilísitico del aprendizaje  en el que hay nodos visibles y otros ocultos o en otra capa, dando así origen al <em>deep learning</em> (aprendizaje profundo), justamente por estas diversas capas de nodos. Esto llevó, según al Comité de Física de los Nobel, a que surgieran aplicaciones exitosas de todo esto, entre ellas, “el reconocimiento de patrones en imágenes, lenguajes y datos clínicos”. </p>\n<p>Pero la cosa se ponía compleja: “si bien ciertas arquitecturas multicapa dieron lugar a aplicaciones exitosas en la década de 1990, siguió siendo un desafío entrenar redes multicapa profundas con muchas conexiones entre capas consecutivas”, señalan, entonces, en los 2000, el propio Hinton -¡junto con colegas!- fue parte de la solución, al proponer un máquina de Boltzmann restringida, una serie de algoritmos de pre entrenamiento que hacía que fuera mucho más rápida y eficiente que la Boltzmann completa.</p>\n<h2>Más allá del Nobel</h2>\n<p>Los aportes de Geoffrey Hinton al desarrollo de la inteligencia artificial van más allá de los reseñados en este reconocimiento de la Real Academia Sueca de Ciencias. Como sucede casi siempre, los trabajos que se reconocen se produjeron hace varias décadas. Y el el caso de Hinton, su perfil desde entonces ha sido tan alto como mediático. Por ejemplo, su renuncia a Google alegando preocupación por algunos riesgos que avizoraba en cómo se estaba implementando la Inteligencia Artificial fue ampliamente cubierta. También suscitó atención que fuera uno de los firmantes de la carta abierta que llamaba a pausar los experimentos gigantes de IA por al menos seis meses](https://ladiaria.com.uy/ciencia/articulo/2024/6/quien-le-teme-a-chatgpt-primera-parte-homo-faber) de marzo de 2023.</p>\n<p>Recientemente, este 2024, Hinton ha hecho declaraciones -incluso fue consultado por el gobierno del Reino Unido, en las que llama a aplicar la Renta Básica Universal, entre otras razones, como una forma de paliar los estragos que produce la pérdida de empleos que trae aparejada la aplicación de la Inteligencia Artificial en diversas áreas. Para ello argumenta que esa aplicación de IA permitirá a quienes la usen aumentar su productividad y ganancias. “La inteligencia artificial va a aumentar la brecha entre los ricos y los pobres”, declaraba al programa <em>Newsnight</em> de la cadena británica BBC, y agregaba que la Renta Básica Universal no sería suficiente dado que “muchas personas obtienen su autoestima por el trabajo que realizan”. Implementar la Renta Básica Universal, razonaba, solucionaría el problema de que no se murieran de hambre o que pudieran pagar su alquiler, “pero no soluciona el problema de la autoestima”. Tal vez este tipo de cosas muestren mejor hoy quién es  Geoffrey Hinton que un brillante premio Nobel.</p>\n<h2>Un Nobel que viene mal</h2>\n<p>Más allá de la tozudez del Premio Nobel por reconocer a personas en particular en lugar de galardonar el trabajo en equipo y cooperativo que hay detrás de la abrumadora mayoría de la producción científica, el galardón tiene un marcado sesgo en reconocer la trayectoria de científicos hombres, blancos y occidentales.</p>\n<p>El año pasado informábamos que al ganar un premio Nobel compartido con dos hombres, <a target=\"_blank\" href=\"https://ladiaria.com.uy/ciencia/articulo/2023/10/con-un-tercio-del-nobel-2023-anne-lhuillier-paso-a-ser-la-quinta-mujer-en-recibir-un-premio-nobel-de-fisica-en-120-anos\">Anne L’Huillier había pasado a ser la quinta mujer en recibir un premio Nobel de Física en 120 años</a>. </p>\n<p>Este año los Nobel de disciplinas científicas no vienen desentonando con su <a target=\"_blank\" href=\"https://ladiaria.com.uy/ciencia/articulo/2023/10/lo-que-dejaron-los-nobel-de-ciencia-de-2023\">triste tradición de tener un marcado sesgo de género</a>. El de <a target=\"_blank\" href=\"https://ladiaria.com.uy/salud/articulo/2024/10/premio-nobel-de-medicina-para-el-descubrimiento-de-los-microarn\">Medicina y Fisiología fue entregado a dos investigadores de Estados Unidos, Victor Ambros y Gary Ruvkun, por sus trabajos con los micro ARN</a>. El de Física fue entregado a un norteamericano y un británico. Sean quienes sean los o las merecedoras del Nobel de Química, que se entrega este miércoles 9 de octubre, será difícil, por no decir imposible, alcanzar una premiación paritaria. Tampoco parece probable que ayuden a deslizar la idea de que quienes hacen ciencia relevante no están solo en los países desarrollados del hemisferio norte.</p>\n          </div>",
  "author": "https://ladiaria.com.uy/periodista/leo-lagos/",
  "favicon": "https://ladiaria.com.uy/static/meta/la-diaria-favicon-16x16.png",
  "source": "ladiaria.com.uy",
  "published": "2024-10-08T17:33:01+00:00",
  "ttr": 310,
  "type": "article"
}