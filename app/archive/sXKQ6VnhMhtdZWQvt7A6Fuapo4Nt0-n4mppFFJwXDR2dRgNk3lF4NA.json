{
  "url": "https://www.elpais.com.uy/vida-actual/viudo-por-muerte-informatica-o-por-que-es-peligroso-que-la-inteligencia-artificial-hable-como-scarlett-johansson",
  "title": "Viudo por muerte informática o por qué es peligroso que la inteligencia artificial hable como Scarlett Johansson",
  "description": "Los usuarios de inteligencia artificial creen que la mitad de las personas simularán sus matrimonios con estos sistemas. Ventajas e inconvenientes de la interacción entre humanos y robots.",
  "links": [
    "https://www.elpais.com.uy/vida-actual/viudo-por-muerte-informatica-o-por-que-es-peligroso-que-la-inteligencia-artificial-hable-como-scarlett-johansson"
  ],
  "image": "https://imgs.elpais.com.uy/dims4/default/a5cb25c/2147483647/strip/true/crop/880x428+0+0/resize/1440x700!/quality/90/?url=https%3A%2F%2Fel-pais-uruguay-production-web.s3.us-east-1.amazonaws.com%2Fbrightspot%2Fcf%2F97%2F257c0e314dbebf99ef938f00f963%2Fakihiko-kondo-con-una-muneca.png",
  "content": "<div>\n<p><b><i>Por Raúl Limón/El País de Madrid</i></b><br /><b>Akihiko Kondo</b>, que ha cumplido 41 años el pasado último día de mayo, se casó con el <b>holograma</b> de su cantante virtual favorita, <b>Hatsune Miku</b>, en una ceremonia simbólica hace seis años. Solo dos aniversarios después, Gatebox, la empresa responsable del avatar, dejó el servicio y el joven administrativo de una escuela pública japonesa enviudó por muerte informática. La historia de Kondo, aunque parezca extravagante, no es más que un anticipo de una realidad de consecuencias impredecibles: la sustitución de relaciones personales reales por <a target=\"_blank\" href=\"https://www.elpais.com.uy/noticias/inteligencia-artificial\">agentes robotizados</a> programados para responder al usuario según sus expectativas.</p><p>El investigador y profesor de robótica de la Universidad de Sheffield <b>Tony Prescott</b> ha presentado el libro The Psychology of Artificial Intelligence (<b>La psicología de la inteligencia artificial</b>), con el que sostiene que la<b> IA </b>puede ser un paliativo de la soledad. Pero con riesgos, según admite él y decenas de investigadores.</p><p>Que ChatGPT-4o se presentara con una voz parecida a la de<b> Scarlett Johansson</b> no es casualidad. Los angloparlantes que vieron la película Her, escrita, dirigida y producida por Spike Jonze y galardonada con el Oscar al mejor guion en 2014, tardaron segundos en asociar el nuevo agente (asistente virtual) de<b> Open AI</b> con la actriz, cuya interacción termina enamorando al solitario protagonista.</p><p>El último informe del Laboratorio de Consumo e Industria de Ericsson (Ericsson Consumer &amp; Industry Lab) refleja que “el 50% de los usuarios iniciales de la inteligencia artificial cree que las personas simularán sus matrimonios para anticipar cambios o incluso prever el divorcio”. Un abrumador 71% de estos <b>consumidores de IA </b>cree que esta utilidad será beneficiosa.</p><p>La sustitución de la interacción humana, con todas sus complejidades, por una relación híbrida entre humano y máquina implica muchas ventajas, pero también numerosos riesgos más reales e inmediatos que los reflejados en algunos capítulos de <b>Black Mirror</b>. “Los<b> robots sociales</b> están diseñados específicamente para interacciones personales que involucran emociones y sentimientos humanos. Pueden aportar beneficios, pero también causar daños emocionales a niveles muy básicos”, advierte Matthias Scheutz, director del Laboratorio de Interacción Humano-Robot en la Universidad de Tufts (EE UU).</p><p>La experiencia de Akihiko Kondo es un resumen de esta complejidad y difiere de otras más relacionadas con experiencias artísticas, como The Hybrid Couple de Alicia Framis, que simula un matrimonio con un holograma como reflexión, o Meirivone</p><p>Rocha, una brasileña de 39 años que infló sus seguidores en las redes al difundir su supuesta boda con un muñeco.</p><p>El japonés, en una entrevista a la BBC, relata haber sufrido acoso por parte de compañeros, admite que los amigos a través de internet y los juegos siguen siendo su “comunidad” y confiesa que nunca ha tenido pareja: “He tenido algunos amores no correspondidos en los que yo siempre era rechazado y me hizo descartar la posibilidad de estar con alguien”. Soledad, acoso, dependencia psicológica y tecnológica, habilidades sociales limitadas, celibato involuntario (el peligroso movimiento en la red antesala de la violencia machista), satisfacción artificial de necesidades, compañeros virtuales con apariencia de realidad… La extravagante historia de Kondo abre la puerta a un análisis de las virtudes y peligros de la injerencia de la IA en las relaciones personales.</p><div>\n<p>\n</p><h3>Ventajas</h3>\n<p></p>\n</div><p>No son pocos ni simples los beneficios de una inteligencia artificial humanizadas.</p><div>\n<p>\n</p><h3>Soledad</h3>\n<p></p>\n</div><p>Prescott admite los riesgos, pero destaca una de las principales ventajas: “En una época en la que muchas personas describen sus vidas como solitarias, puede ser valioso tener la compañía de la IA como una forma de interacción social recíproca que sea estimulante y personalizada. La soledad humana a menudo se caracteriza por una espiral descendente en la que el aislamiento conduce a una autoestima más baja, lo que desalienta una mayor interacción con las personas. El compañerismo de la IA podría ayudar a romper este ciclo como andamiaje de la autoestima y ayudar a mantener o mejorar las habilidades sociales. Si es así, las relaciones con las IA podrían ayudar a las personas a encontrar compañía tanto con humanos como con otros artificiales”.</p><div>\n<figure>\n<picture>\n<img alt=\"Tony Prescott\" srcset=\"https://imgs.elpais.com.uy/dims4/default/77e6790/2147483647/strip/true/crop/600x600+0+0/resize/600x600!/quality/90/?url=https%3A%2F%2Fel-pais-uruguay-production-web.s3.us-east-1.amazonaws.com%2Fbrightspot%2F11%2Fca%2Fa1edbc364f718033c41b914efed8%2Ftony-prescott.jpg 1x, https://imgs.elpais.com.uy/dims4/default/f4d2390/2147483647/strip/true/crop/600x600+0+0/resize/1200x1200!/quality/90/?url=https%3A%2F%2Fel-pais-uruguay-production-web.s3.us-east-1.amazonaws.com%2Fbrightspot%2F11%2Fca%2Fa1edbc364f718033c41b914efed8%2Ftony-prescott.jpg 2x\" src=\"https://imgs.elpais.com.uy/dims4/default/77e6790/2147483647/strip/true/crop/600x600+0+0/resize/600x600!/quality/90/?url=https%3A%2F%2Fel-pais-uruguay-production-web.s3.us-east-1.amazonaws.com%2Fbrightspot%2F11%2Fca%2Fa1edbc364f718033c41b914efed8%2Ftony-prescott.jpg\" />\n</picture>\n<div>\n<figcaption>Investigador y profesor de robótica de la Universidad de Sheffield, Tony Prescott</figcaption><p>Foto: Universidad de Sheffield.</p></div>\n</figure>\n</div><div>\n<p>\n</p><h3>Cuidado</h3>\n<p></p>\n</div><p><b>Joan Claire Tronto</b>, profesora de ciencias políticas en la Universidad de Minnesota, amplía el concepto de cuidado (care) a todo aquello que “hacemos para mantener, continuar y reparar nuestro mundo con el fin de que podamos vivir en él lo mejor posible”. En su trabajo, una clave es “el compromiso de satisfacer las necesidades de los demás”. Y la IA puede hacerlo sin descanso. Luis Merino es catedrático de la Universidad Pablo de Olavide y responsable de robótica social, la disciplina destinada a asistencia de colectivos humanos de forma autónoma y con capacidad de aprender de las emociones de los destinatarios de los servicios: “El objetivo es que los robots entiendan nuestras intenciones y emociones y aprendan de ellas”.</p><div>\n<p>\n</p><h3>Beneficio o interés</h3>\n<p></p>\n</div><p>El máximo responsable de Open AI, Sam Altman, describe a su último modelo como “colega supercompetente”. El primer término hace referencia a su humanización y el segundo a los beneficios que aporta por la ejecución de tareas en nombre del usuario. Esta última ventaja aporta “bienestar individual”, según <b>Brad Hooker</b>, profesor de filosofía en la Universidad de Reading. Este interés es inherente a la interacción humana. No siempre se busca un beneficio, pero es difícil que una relación prospere si los costes superan constantemente a los beneficios.</p><div>\n<p>\n</p><h3>Desarrollo humano</h3>\n<p></p>\n</div><p>La IA puede promover actitudes y conductas que faciliten la realización personal y la interacción con los demás. En una evaluación de ChatGPT, Gemini y Llama (Meta), la Universidad de Illinois ha mostrado la importancia de esta habilidad. “Los agentes pueden ayudar a aumentar, por ejemplo, la conciencia sobre los comportamientos saludables, a comprometerse emocionalmente con los cambios y a darse cuenta de cómo sus hábitos podrían afectar a las personas que los rodean”, explica Michelle Bak, investigadora de los modelos.</p><div>\n<p>\n</p><h3>Autonomía</h3>\n<p></p>\n</div><p>Se refiere al potencial de la IA para aportar información relevante para que el individuo actúe y decida de acuerdo con sus propias motivaciones e intereses.</p><div>\n<p>\n</p><h3>Riesgos</h3>\n<p></p>\n</div><p>De cada una de estas categorías de ventajas surgen ramas de riesgos asociados. Estos son algunos destacados por los investigadores:</p><div>\n<p>\n</p><h3>Daños físicos o emocionales</h3>\n<p></p>\n</div><p>Las anécdotas de los primeros modelos de IA amenazando, insultando o promoviendo comportamientos dañinos o violentos no son nuevas, aunque periódicamente alimenten desproporcionadas e insustanciales reacciones. Hace un año que la cadena de supermercados Pak ‘n’ Save de <b>Nueva Zelanda</b> fue advertida porque su IA planificadora de menús recomendaba bebidas con cloro gaseoso y bocadillos con veneno y pegamento. Obviamente, nadie siguió estos consejos porque el sentido común prima, pero puede haber casos menos evidentes y extremos.</p><p>Amelia Glaese, investigadora de Google DeepMind y ahora de Open AI, busca fórmulas y sistemas para evitar estos accidentes. “Utilizamos el aprendizaje por refuerzo a partir de la retroalimentación humana para (…) que nuestro agente sea más útil e inofensivo y proporcione evidencias de las fuentes que respaldan las afirmaciones”.</p><p>La <b>humanización del robot</b> con empatía y herramientas de voz y vídeo añade peligrosidad al ofrecer una interacción más realista e inmersiva y hacer creer al usuario que está con un amigo o interlocutor de confianza. Una aplicación extrema puede ser la tentación de mantener una versión virtual de un ser querido fallecido y evitar así el duelo necesario para continuar la vida.</p><p>Los investigadores reclaman que estos desarrollos se prueben en circuitos cerrados (sandbox) antes de ofrecerse, se supervisen y evalúen de forma constante, se analice la variedad de daños que pueden causar en distintos ámbitos y se prevean fórmulas para mitigarlos.</p><div>\n<p>\n</p><h3>Limitación del desarrollo personal</h3>\n<p></p>\n</div><p>“Algunos usuarios buscan con sus compañeros de IA relaciones que estén libres de los obstáculos, sin opiniones, preferencias y normas que pueden entrar en conflicto con las suyas”, advierte un estudio de media docena de universidades para DeepMind. Y, además, con un lenguaje adulador.</p><p>Shannon Vallor, filósofa especializada en ética de la ciencia e inteligencia artificial, advierte sobre el peligro de que los nuevos sistemas promuevan relaciones “sin fricciones”, pero también sin valores: “No tienen la vida mental y moral que los humanos tenemos detrás de nuestras palabras y acciones”.</p><p>Ese tipo de relación supuestamente ideal, según estos expertos, desalienta la necesidad de cuestionarnos y avanzar en el desarrollo personal al mismo tiempo que promueve la renuncia a la interacción real y generar dependencia de esas máquinas dispuestas a halagar y buscar una satisfacción a corto plazo.</p><div>\n<p>\n</p><h3>Manipulación</h3>\n<p></p>\n</div><p>Esa dependencia emocional de un sistema capaz de persuadir es el acceso inmediato a la interferencia en los comportamientos, intereses, preferencias, creencias y valores de los usuarios, a su capacidad para tomar decisiones libres e informadas. “Las emociones que los usuarios sienten hacia sus asistentes podrían ser explotadas para manipularlos o, llevadas al extremo, coaccionarlos para creer, elegir o hacer algo que, de otro modo, no habrían creído, elegido o hecho”, advierte el documento de DeepMind.</p><div>\n<p>\n</p><h3>Dependencia material</h3>\n<p></p>\n</div><p>El final de la experiencia de Akihiko Kondo y su matrimonio virtual con un holograma es un claro ejemplo. Fue la empresa responsable de la programación y mantenimiento del sistema la que puso fin a la solución que encontró el administrativo japonés para satisfacer ciertas necesidades. Los desarrolladores pueden generar la dependencia y luego interrumpir la tecnología por la dinámica del mercado o los cambios normativos sin tomar las medidas adecuadas para mitigar los posibles daños al usuario.</p>\n</div>",
  "author": "",
  "favicon": "https://www.elpais.com.uy/favicon-16x16.png",
  "source": "elpais.com.uy",
  "published": "2024-06-10T19:25:52.804",
  "ttr": 309,
  "type": "article"
}