{
  "url": "https://www.montevideo.com.uy/Ciencia-y-Tecnologia/Los-grandes-modelos-de-lenguaje-de-IA-son-cada-vez-menos-fiables-advierte-estudio-uc901415",
  "title": "Los grandes modelos de lenguaje de IA son cada vez menos fiables, advierte estudio",
  "description": "Un estudio realizado por la Universitat Politècnica de València (UPV) y la Universidad de Cambridge revela la tendencia \"alarmante\" al empeoramiento de la fiabIlidad de los modelos más recientes de...",
  "links": [
    "https://www.montevideo.com.uy/Ciencia-y-Tecnologia/Los-grandes-modelos-de-lenguaje-de-IA-son-cada-vez-menos-fiables-advierte-estudio-uc901415",
    "https://www.montevideo.com.uy/Ciencia-y-Tecnologia/Los-grandes-modelos-de-lenguaje-de-IA-son-cada-vez-menos-fiables-advierte-estudio-uc901415?plantilla=1685&proceso=amp",
    "https://www.montevideo.com.uy/auc.aspx?901415="
  ],
  "image": "https://imagenes.montevideo.com.uy/imgnoticias/202301/_W933_80/829384.jpg",
  "content": "<div>\n                              \t<p>Un estudio realizado por la Universitat Politècnica de\nValència (UPV) y la Universidad de Cambridge revela la tendencia\n\"alarmante\" al empeoramiento de la fiabIlidad de los modelos más\nrecientes de inteligencia artificial (IA), como el GPT-4, en comparación con\nlos primeros, como el GPT-3.</p><p>El trabajo, que se publica esta semana en la revista<em>\nNature</em>, señala que los recientes avances en IA han generalizado el uso de\ngrandes modelos de lenguaje en nuestra sociedad, pero no son tan fiables como\nlos usuarios esperan, según detalla la UPV.</p><p>El estudio fue liderado por un equipo del Instituto VRAIN de\nla Universitat Politècnica de València y la Escuela Valenciana de Posgrado y\nRed de Investigación en Inteligencia Artificial (ValgrAI), junto con la\nUniversidad de Cambridge.</p><p>El equipo integrado en el instituto VRAIN de la UPV fue\nparte del 'red team' de GPT-4, cuya misión era encontrar fallos y\nvulnerabilidades en el sistema, así como evaluar sus capacidades y posibles\nriesgos asociados a su uso.</p><p>Trabajaron en el estudio los investigadores de la UPV José\nHernández-Orallo, Cèsar Ferri, Wout Schellaert, Lexin Zhou y Yael Moros.</p><p><strong>Percepción humana de dificultad</strong></p><p>Según Hernández-Orallo, una de las principales\npreocupaciones sobre la fiabilidad de los modelos de lenguaje es que su\nfuncionamiento no se ajusta a la percepción humana de dificultad de la tarea.</p><p>Es decir, existe una discordancia entre las expectativas de\nque los modelos fallen de acuerdo con la percepción humana de dificultad en la\ntarea y las tareas donde realmente los modelos fallan, precisa.</p><p>\"Los modelos pueden resolver ciertas tareas complejas de\nacuerdo con las habilidades humanas, pero al mismo tiempo fallan en tareas\nsimples del mismo dominio. Por ejemplo -apunta-, pueden resolver varios\nproblemas matemáticos de nivel de doctorado, pero se pueden equivocar en una\nsimple suma\".</p><p>El equipo de la UPV y la Universidad de Cambridge investigó\ntres aspectos clave que afectan a la fiabilidad de los modelos de lenguaje\ndesde una perspectiva humana, y concluyen que no existe una \"zona\nsegura\" en la que los modelos funcionen a la perfección.</p><p>\"Los modelos suelen ser menos precisos en tareas que\nlos humanos consideran difíciles, pero no son precisos al 100 % ni siquiera en\ntareas sencillas. Esto significa que no existe una 'zona segura' en la que se\npueda confiar en que los modelos funcionen a la perfección\", agrega Yael\nMoros Daval.</p><p>De hecho, los modelos más recientes básicamente mejoran su\nrendimiento en tareas de alta dificultad, pero no en tareas de baja dificultad,\nlo que \"agrava la discordancia de dificultad entre el rendimiento de los\nmodelos y las expectativas humanas\", precisa Martínez Plumed.</p><p>El estudio descubre asimismo que los modelos de lenguaje\nrecientes son mucho más propensos a proporcionar respuestas incorrectas, en vez\nde evitar dar respuesta a tareas de las que no están seguros, lo que puede\nllevar a que los usuarios que inicialmente confían demasiado en los modelos\nluego \"se decepcionen\", añade Lexin Zhou.</p><p><strong>Sensibilidad al enunciado del problema</strong></p><p>El estudio analiza asimismo si la eficacia de la formulación\nde las preguntas se ve afectada por la dificultad de las mismas y concluye que\nes posible que la tendencia actual de progreso en el desarrollo de modelos de\nlenguaje y de mayor comprensión de una variedad de órdenes no libere a los\nusuarios de preocuparse en hacer enunciados eficaces.</p><p>\"Hemos comprobado que los usuarios pueden dejarse\ninfluir por 'prompts' que funcionan bien en tareas complejas pero que, al mismo\ntiempo, obtienen respuestas incorrectas en tareas sencillas\", agrega César\nFerri.</p><p>Además de estos hallazgos sobre aspectos de la falta de\nfiabilidad de los modelos de lenguaje, los investigadores han descubierto que\nla supervisión humana es incapaz de compensar estos problemas.</p><p>Por ejemplo, las personas pueden reconocer las tareas de\nalta dificultad, pero siguen considerando con frecuencia que los resultados\nincorrectos son correctos en esta área, incluso cuando se les permite decir\n\"no estoy seguro\", lo que indica un exceso de confianza.</p><p><strong>Desde ChatGPT a LLaMA y BLOOM</strong></p><p>Los resultados fueron similares para múltiples familias de\nmodelos de lenguaje, incluidos la GPT de OpenAI, LLaMA de pesos abiertos de\nMeta, y BLOOM, una iniciativa totalmente abierta de la comunidad científica.</p><p>Los investigadores constataron que los problemas de\ndiscordancia de dificultad, falta de abstención adecuada y sensibilidad al\n'prompt' siguen siendo un problema para las nuevas versiones de las familias\npopulares como los nuevos modelos o1 de OpenAI y Claude-3.5-Sonnet de\nAnthropic.</p><p>Los investigadores proponen un cambio en el diseño y\ndesarrollo de la IA de propósito general, sobre todo para las aplicaciones de\nalto riesgo, en las que la predicción del desempeño de los modelos de lenguaje\ncomo la detección de sus errores son primordiales.</p><p>EFE</p>\n                                </div>",
  "author": "@portalmvd",
  "favicon": "https://www.montevideo.com.uy/favicon/favicon.ico",
  "source": "montevideo.com.uy",
  "published": "",
  "ttr": 151,
  "type": "article"
}