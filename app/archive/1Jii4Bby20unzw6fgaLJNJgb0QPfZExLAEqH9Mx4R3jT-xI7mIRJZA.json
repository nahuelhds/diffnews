{
  "url": "https://www.elobservador.com.uy/nota/los-condicionantes-sociales-en-el-desarrollo-e-implementacion-de-la-inteligencia-artificial-20242110380",
  "title": "Los condicionantes sociales en el desarrollo e implementación de la Inteligencia Artificial",
  "description": "La Inteligencia Artificial (IA) promete estar nuevamente en la agenda en la próxima cumbre del G7 en el verano europeo. Y con razón: el impacto de la IA en la humanidad eclipsará los cambios tecnológicos...",
  "links": [
    "https://www.elobservador.com.uy/nota/los-condicionantes-sociales-en-el-desarrollo-e-implementacion-de-la-inteligencia-artificial-20242110380",
    "https://www.elobservador.com.uy/nota/los-condicionantes-sociales-en-el-desarrollo-e-implementacion-de-la-inteligencia-artificial-20242110380/amp"
  ],
  "image": "https://cdn.elobservador.com.uy/022024/1706794732413.jpg?&cw=600&ch=365",
  "content": "<p>La Inteligencia Artificial (IA) promete estar nuevamente en la agenda en la próxima cumbre del G7 en el verano europeo. Y con razón: el impacto de la IA en la humanidad eclipsará los cambios tecnológicos anteriores. La inteligencia artificial ofrece un enorme potencial y un riesgo inmenso. En consecuencia, su regulación es un asunto importante para los gobiernos a nivel nacional e internacional, según escribieron en un artículo para The Atomic Scientists Bullein las científicas Elena Simperl y Johanna Walker.</p><div><p>En la cumbre de noviembre en Reino Unido, países de todo el mundo (incluidos Estados Unidos y China) firmaron la Declaración de Bletchley. En él, las naciones se comprometieron a abordar juntas la “IA de frontera”, identificando los riesgos de seguridad de la IA “de frontera” y colaborando en políticas para mitigar estos riesgos. La IA de frontera se define como “modelos de IA de uso general altamente capaces que pueden realizar una amplia variedad de tareas e igualar o superar las capacidades presentes en los modelos más avanzados de la actualidad”.</p><p>Sin lugar a duda, se necesita colaboración internacional cuando se trata de garantizar la seguridad de los sistemas de IA, por lo que el hecho de que la declaración tuviera un alcance global e incluyera a naciones como China, y se basara en el trabajo intergubernamental en curso de la OCDE, GPAI (Asociación Mundial sobre Inteligencia Artificial) y otros foros, es alentador. Si se puede llegar a un acuerdo sobre la IA en un entorno internacional como la próxima Cumbre del G7, también se podrán ver cambios reales a nivel nacional.</p><p>Sin embargo, dicen las autoras, un acuerdo sobre IA de frontera, que se ocupa principalmente de grandes modelos lingüísticos, por sí solo no es suficiente. Actualmente, la atención se centra en la IA de frontera en lugar de la IA fundamental (modelos que fueron entrenados para realizar una amplia variedad de tareas).</p><p>Además, las preocupaciones en torno a los riesgos existenciales de la IA de frontera son, en el mejor de los casos, exageradas y, en el peor, una distracción. En cambio, la regulación debería exigir que los conceptos de seguridad y responsabilidad estén arraigados en todos los sistemas de IA, de modo que aborden las desigualdades sociales que condicionan los datos de capacitación, a quienes ingresan datos y a quienes dependen de los sistemas de IA.</p><p>Dicha regulación también debería incluir disposiciones para educar al público a medida que estos sistemas continúan desarrollándose, de modo que los usuarios finales comprendan las capacidades y limitaciones de las aplicaciones de IA o se vuelvan más alfabetizados en IA.</p><p>Nuevas formas de IA generativa, principalmente ChatGPT de OpenAI y Bard de Google, han llegado a dominar los titulares de los medios recientemente. Sin embargo, toda una gama de sistemas de IA muy potentes utilizados durante la última década e incluso en años anteriores ya habían causado daños reales, exacerbando las desigualdades existentes en la sociedad.</p><p>Por ejemplo, existen importantes sesgos de edad y raza en los sistemas de detección de vehículos autónomos: es más probable que una persona sea atropellada por un vehículo autónomo si es joven y negra, a diferencia de una persona blanca y de mediana edad. Los sesgos de edad y raza existen en los vehículos autónomos porque los datos utilizados por las empresas de automóviles para entrenar sus modelos a menudo no son representativos y están sesgados hacia los blancos de mediana edad.</p><p>Simperl y Walker afirman que los estudios también han demostrado que algunos sistemas de reconocimiento facial de IA, que han existido de alguna forma desde la década de 1960 pero que se han utilizado mucho más en este siglo, detectaban a las mujeres negras como personas con un nivel mucho mayor de precisión cuando llevaban una máscara blanca, lo que demuestra que un sesgo hacia los hombres blancos. Los sistemas de detección de peatones asocian ser blanco con ser peatón, lo que genera resultados injustos y potencialmente mortales.</p><p>La regulación de la IA debería abordar no sólo el sesgo de los datos, sino también su idoneidad. Si entrena un modelo de IA con datos que no son representativos, de mala calidad o que no coinciden con la función de un modelo, los resultados se verán afectados. Tampoco es suficiente que los datos sean limpios o representativos: también deben ser adecuados para el propósito previsto.</p><div><p><img src=\"https://media.cdnp.elobservador.com.uy/022024/1706794775677/ia%202.webp?&amp;extw=jpeg&amp;cw=1024\" /></p><p>YouTube</p></div><p>En un experimento reciente, se pidió a un modelo GPT-3.5 entrenado en 140.000 mensajes del canal Slack que escribiera contenido. El sistema respondió: \"Trabajaré en eso por la mañana\". La respuesta reflejó lo que decían los usuarios en sus chats de trabajo cuando se les preguntó lo mismo. En lugar de escribir correos electrónicos, blogs y discursos según lo solicitado, el modelo se hizo eco de las preocupaciones y la naturaleza de su conjunto de datos, posponiéndolo para el día siguiente. Al utilizar un conjunto de datos fundamentalmente inadecuado, aunque superficialmente parecía apropiado, el modelo desempeñó una función completamente diferente a la prevista.</p><p>Según las autoras del artículo, para evitar estos obstáculos y peligros, los expertos deben adoptar una visión de la IA más centrada en los datos. Esto significa aplicar también los principios de mejores prácticas utilizados en la gestión de riesgos en los modelos de IA a los conjuntos de datos en los que están entrenados. Luego deben tomar medidas activas para establecer las herramientas, estándares y mejores prácticas para garantizar que estos datos sean precisos, representativos y libres de sesgos.</p><p>​ La forma en que se maneja o entrena la IA es una cuestión social, además de tecnológica. Las organizaciones involucradas en la creación o el enriquecimiento de datos deben incorporar grupos subrepresentados para garantizar que estén tomando decisiones justas y que sus modelos puedan usarse de manera segura y responsable.</p><p>Además, este enfoque ético debe estar arraigado en toda la cadena de valor de los datos. Para las juntas ejecutivas esto significa comprender (y mitigar) el impacto social de sus productos. Para los trabajadores autónomos que en realidad etiquetan datos para entrenar sistemas de inteligencia artificial, esto significa condiciones de trabajo más justas. Para incorporar la equidad para los usuarios finales de la IA será necesario regular las prácticas a lo largo de esta cadena.</p><p>La demanda de datos fiables y justos para fines de formación en IA es urgente. Los investigadores predicen que las grandes empresas tecnológicas se quedarán sin datos de alta calidad para 2026. Por lo tanto, la necesidad de datos representativos, éticos y utilizables será cada vez más apremiante. Grupos colectivos de ingeniería que abarcan tanto la academia como la industria, como ML Commons, un consorcio de ingeniería de inteligencia artificial, están trabajando para crear conjuntos de datos abiertos y diversos para su uso en las aplicaciones comerciales con las que es más probable que interactúen los usuarios finales. Pero hay mucho más por hacer.</p><p>Entonces, ¿qué pasa con los marcos existentes de ética de la IA? Organizaciones como la UNESCO han establecido principios éticos para los sistemas de IA en el diseño, desarrollo e implementación, pero los estudios con ingenieros de IA muestran una y otra vez que hay demasiada ambigüedad en cómo ponerlos en práctica.</p><p>Los estándares globales sobre ética de la IA, como la “Recomendación sobre la ética de la inteligencia artificial” de la UNESCO de 2021, ratificada por los 193 estados miembros, buscan consagrar un “enfoque de derechos humanos para la IA” basado en principios de responsabilidad, transparencia, privacidad y explicabilidad.</p><p>Los expertos han intentado capacitar a los profesionales de la IA para que auditen su trabajo según los marcos éticos establecidos. Esto incluye incorporar un seguimiento ético activo junto con la normativa legal existente, como el Reglamento General de Protección de Datos. Sin embargo, la escasa financiación de la investigación sugiere que hasta ahora el apoyo gubernamental no se ha extendido desde el establecimiento de marcos hasta su puesta en práctica. Para garantizar que la regulación de la IA pueda ir más allá del anuncio de un nuevo marco, es necesario trabajar a nivel operativo.</p><p>Antes de la reunión del G7 de 2024, los legisladores que estén pensando en una regulación eficaz de la IA también deberían centrarse en la alfabetización en IA. Los sistemas de IA tienen una presencia cada vez mayor en la vida humana, ya sea en sistemas automatizados de recursos humanos, vehículos autónomos o IA generativa como ChatGPT.</p><p>Pero ¿saben realmente los usuarios cómo funcionan estos sistemas o cuáles podrían ser los costos ocultos detrás de ellos?</p><p>A medida que el uso de la IA se generaliza, se requiere más energía para entrenar y ejecutar estos sistemas cada vez más grandes. Una investigación de la Universidad de Pensilvania sugiere que, si la humanidad continúa con su trayectoria actual de uso de IA, para 2030 la electricidad global consumida por las computadoras podría aumentar entre un 8 y un 21 por ciento, exacerbando la actual crisis energética.</p><p>Un estudio incluso encontró que el entrenamiento de un modelo de lenguaje grande generó casi 300 toneladas de dióxido de carbono, aproximadamente la cantidad emitida en 125 vuelos de ida y vuelta entre la ciudad de Nueva York y Beijing.</p><p>De manera similar, cuando las personas utilizan la IA, a menudo no son conscientes de cómo llega a sus conclusiones, algunas de las cuales podrían contener inexactitudes. Cada vez más, se utiliza la inteligencia artificial para tomar decisiones sobre si un individuo puede solicitar una hipoteca o estará cubierto por un seguro médico, pero el consumidor promedio no tiene claro cuándo ocurre exactamente esto y cómo llega el modelo a sus conclusiones. y mucho menos si esas conclusiones son justas.</p><p>Es trabajo de los profesionales de la IA es ayudar a las personas en este viaje, comunicar claramente el alcance y los beneficios de las tecnologías de IA y ser transparentes sobre sus costos. Capacitar a las personas para que utilicen la IA con confianza ayudará a que su adopción se generalice lo suficiente como para garantizar que los beneficios realmente lleguen a todos.</p><p>Simperl y Walker estiman que, en última instancia, la Cumbre del G7 es una vez más una oportunidad para que los líderes mundiales logren una regulación correcta de la IA. Para hacerlo, deben ir más allá del resplandor y el ruido de los grandes modelos lingüísticos para abordar los sistemas de inteligencia artificial que ya causan daño en la actualidad. Esto implicará mirar más allá de los aspectos técnicos de la IA, sino también sus aspectos sociales y cómo los seres humanos como sociedad quieren integrar la diversidad, la confianza, la seguridad y la equidad en los conjuntos de datos que utilizan.</p><p>La tarea es difícil, pero si no se hace, los sistemas de inteligencia artificial de este año y el próximo no cumplirán las promesas de un mundo mejor.</p><p><em>(Extractado de The Atomic Scientists Bulletin)</em></p></div>",
  "author": "Redacción",
  "favicon": "https://www.elobservador.com.uy/images/favicons/favicon-16x16.png",
  "source": "elobservador.com.uy",
  "published": "2024-02-01T13:40:37.572Z",
  "ttr": 354,
  "type": "article"
}