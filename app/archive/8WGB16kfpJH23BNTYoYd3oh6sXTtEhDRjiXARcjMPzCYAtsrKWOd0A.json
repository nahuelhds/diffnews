{
  "url": "https://www.elobservador.com.uy/nota/la-regulacion-de-las-armas-de-inteligencia-artificial-una-urgencia-imprescindible-20241141200",
  "title": "La regulación de las armas de Inteligencia Artificial: una urgencia imprescindible",
  "description": "El 27 de noviembre de 2020, agentes de inteligencia israelíes asesinaron al científico nuclear iraní Mohsen Fakhrizadeh y su esposa que habían abandonado la costa del Mar Caspio y viajaban en un convoy de...",
  "links": [
    "https://www.elobservador.com.uy/nota/la-regulacion-de-las-armas-de-inteligencia-artificial-una-urgencia-imprescindible-20241141200",
    "https://www.elobservador.com.uy/nota/la-regulacion-de-las-armas-de-inteligencia-artificial-una-urgencia-imprescindible-20241141200/amp"
  ],
  "image": "https://cdn.elobservador.com.uy/012024/1705244619301.jpg?&cw=600&ch=365",
  "content": "<p>El 27 de noviembre de 2020, agentes de inteligencia israelíes asesinaron al científico nuclear iraní Mohsen Fakhrizadeh y su esposa que habían abandonado la costa del Mar Caspio y viajaban en un convoy de cuatro coches hacia su casa familiar en el campo iraní Cuando se acercaban a un giro en U, una lluvia de balas destrozó sus parabrisas y alcanzó a Fakhrizadeh que murió en el acto.</p><div><p>El agente israelí que llevó a cabo el asesinato no tuvo que huir del lugar: el tirador había utilizado una ametralladora operada a distancia, disparada desde más de 1.000 kilómetros millas de distancia. El Mossad había personalizado un fusil automático fabricado en Bélgica con un aparato robótico avanzado que cabía en la caja de una camioneta y estaba equipado con un grupo de cámaras, proporcionando una visión completa del objetivo y el entorno circundante. Para tener en cuenta el retraso en la transmisión de señales al arma, el Mossad utilizó software de inteligencia artificial (IA) para tener en cuenta el retraso, la sacudida del camión provocada cuando se disparaba cada bala y la velocidad del vehículo de Fakhrizadeh.</p><p>En lugar de ser un caso atípico, la operación fue el anticipo ha sido un presagio de la innovación futura. Las naciones, tanto grandes como pequeñas, se apresuran a adquirir drones avanzados, incorporar análisis algorítmicos de objetivos y desarrollar una variedad de armas autónomas terrestres y marítimas, todo ello con poca supervisión o restricción. Como tal, es urgente que los países acuerden reglas comunes sobre el desarrollo, despliegue y uso de estas herramientas en la guerra, según indica Steve Felsdtein, un experto en tecnología, democracia y derechos humanos de la Fundación Carnegie para la Paz internacional en Democracia.</p><p>Para mejorar la supervisión y la previsibilidad, los expertos y los responsables de la formulación de políticas deberían considerar qué medidas podrían tomar las principales potencias de la IA. Estados Unidos podría liderar el camino comprometiéndose a supervisar su propio desarrollo de armas de inteligencia artificial. También podría asociarse con otras naciones para crear un grupo de monitoreo de expertos independientes que vigilaría cómo se utiliza la IA en la guerra. Por último, los países deberían sentarse a la mesa para decidir las normas de uso de la tecnología militar emergente, antes de que sea demasiado tarde.</p><p>Los sistemas de IA relevantes para la seguridad nacional abarcan una variedad de aplicaciones, pero pueden clasificarse en términos generales en tareas “ascendentes” (inteligencia, vigilancia y reconocimiento; comando y control; gestión de la información; logística y capacitación) y tareas “descendentes” (selección y compromiso de objetivos). Concretamente, las aplicaciones de IA permiten a los militares una mayor capacidad analítica (para agregar y analizar datos del campo de batalla y mejorar la capacidad operativa) para ataques con misiles y para el despliegue de drones autónomos impulsados por IA.</p><p>Según Feldstein, algunos expertos sostienen que Estados Unidos no puede permitirse el lujo de obstaculizar el progreso hacia el desarrollo de armas totalmente autónomas por temor a que los chinos o los rusos superen sus esfuerzos. Y, sin duda, las capacidades de IA están proliferando rápidamente. Como lo demuestran la guerra de Ucrania y las hostilidades en Gaza, sin un marco común y limitaciones acordadas, los Estados corren el riesgo de una carrera hacia el fondo, desplegando sucesivamente sistemas más destructivos con escasas restricciones.</p><p><strong>De Ucrania a Gaza</strong></p><p>La actual guerra en Ucrania ha sido descrita como un “súper laboratorio de invención” que ha brindado a las empresas tecnológicas y a los empresarios la oportunidad de probar nuevas herramientas directamente en el campo de batalla. El conflicto ha revelado un cambio importante en la forma de librar la guerra. Uno de los cambios más importantes ha sido la introducción de sistemas integrados de gestión de batalla que ofrecen transparencia actualizada al minuto sobre los movimientos y ubicaciones de las tropas, hasta los niveles básicos de las unidades. “Hoy en día, una columna de tanques o una columna de tropas que avanzan pueden ser descubiertas en tres a cinco minutos y atacadas en otros tres minutos”, advierte el general de división Vadym Skibitsky, alto funcionario del servicio de inteligencia militar de Ucrania. \"La capacidad de supervivencia en movimiento no supera los 10 minutos\".</p><p>Feldstein señala que la línea del frente de Ucrania se ha visto inundada de vehículos aéreos no tripulados, que no sólo proporcionan un seguimiento constante de los acontecimientos en el campo de batalla, sino que, cuando se combinan con sistemas de puntería impulsados por IA, también permiten la destrucción casi instantánea de activos militares. Naturalmente, tanto los rusos como los ucranianos han recurrido a la guerra electrónica contra drones para anular el impacto de los vehículos aéreos no tripulados. Pero esto ha dado paso a otro acontecimiento: un rápido impulso hacia la plena autonomía. Como dijo el erudito militar T.X. Hammes: “Los drones autónomos no tendrán el vulnerable enlace de radio con los pilotos, ni necesitarán guía GPS. La autonomía también aumentará enormemente la cantidad de drones que se pueden emplear al mismo tiempo”.</p><p>La IA militar también está dando forma a la guerra en Gaza. Después de que los militantes de Hamas sorprendieron a las fuerzas de Israel al neutralizar las capacidades de vigilancia de alta tecnología del “Muro de Hierro” del país (una barrera física de 65 kilómetros de largo equipada con cámaras de video inteligentes, sensores guiados por láser y radar avanzado), Israel ha retomado la iniciativa tecnológica.</p><div><p><img src=\"https://media.cdnp.elobservador.com.uy/012024/1705244708326/Ucrania,%20drone.%20Picryl.webp?&amp;extw=jpg&amp;cw=1024\" /></p><p>Picryl</p></div><p>El experto señala que las Fuerzas de Defensa de Israel (FDI) han estado utilizando una plataforma de objetivos de IA conocida como “el Evangelio” (Gospel) . Según los informes, el sistema está desempeñando un papel central en la invasión en curso, produciendo \"recomendaciones automáticas\" para identificar y atacar objetivos. El sistema se activó por primera vez en 2021, durante la guerra de 11 días de Israel contra Hamás. Para el conflicto de 2023, las FDI estiman que han atacado 15.000 objetivos en Gaza en los primeros 35 días de la guerra. (En comparación, Israel atacó entre 5.000 y 6.000 objetivos en el conflicto de Gaza de 2014, que duró 51 días). Si bien el Evangelio ofrece capacidades militares críticas, el número de víctimas civiles es preocupante. Una fuente describe la plataforma como una “fábrica de asesinatos en masa” con énfasis en la cantidad de objetivos sobre la calidad de los mismos. También existe el riesgo de que la dependencia de Israel de la IA esté dando lugar a un “sesgo de automatización”, en el que los operadores humanos están predispuestos a aceptar recomendaciones generadas por máquinas en circunstancias en las que los humanos habrían llegado a conclusiones diferentes.</p><p><strong>¿Es posible un consenso internacional sobre la IA?</strong></p><p>Como lo atestiguan las guerras en Ucrania y Gaza, los ejércitos rivales se apresuran a desplegar herramientas automatizadas a pesar del escaso consenso sobre los límites éticos para el despliegue de tecnologías no probadas en el campo de batalla. Mi investigación muestra que potencias líderes como Estados Unidos están comprometidas a aprovechar “sistemas autónomos y accesibles en todos los ámbitos”. En otras palabras, los principales ejércitos están repensando los preceptos fundamentales sobre cómo se libra la guerra y apoyándose en las nuevas tecnologías. Estos acontecimientos son especialmente preocupantes a la luz de numerosas preguntas sin resolver: ¿Cuáles son exactamente las reglas cuando se trata de utilizar drones autónomos letales o ametralladoras robóticas en zonas pobladas? ¿Qué salvaguardias se requieren y quién es culpable si se daña a civiles?</p><p>Para Feldstein, a medida que más y más países se convenzan de que las armas de IA son la clave para el futuro de la guerra, se verán incentivados a invertir recursos en el desarrollo y la proliferación de estas tecnologías. Si bien puede resultar poco práctico prohibir las armas letales autónomas o restringir las herramientas basadas en inteligencia artificial, eso no significa que las naciones no puedan tomar más iniciativas para determinar cómo se utilizan.</p><p>Estados Unidos y las democracias afines deberían impulsar la creación de un grupo de expertos independientes con sanción internacional para monitorear los efectos continuos de las herramientas de inteligencia artificial utilizadas en la guerra. Por ejemplo, si son ciertos los informes de que “el 90 por ciento de los objetivos alcanzados” en el conflicto de Gaza se deben a recomendaciones generadas por IA, entonces corresponde a los formuladores de políticas tener una comprensión más granular de los riesgos y beneficios de tales sistemas. ¿Cuáles son los impactos civiles de estas plataformas de ataque? ¿Qué parámetros se utilizan y qué nivel de supervisión se ejerce sobre los algoritmos de focalización? ¿Qué tipo de procedimientos de rendición de cuentas existen? El propósito del grupo sería destacar áreas de actividad y ofrecer recomendaciones a gobiernos y organizaciones internacionales sobre cómo abordar los problemas emergentes.</p><p>Finalmente, los estados deberían acordar establecer un piso de conducta sobre cómo los militares utilizarán las tecnologías emergentes en la guerra. El riesgo es que los países, en particular los regímenes no democráticos, inicien una carrera hacia el abismo, utilizando combinaciones de herramientas cada vez más letales para lograr efectos destructivos. Los gobiernos podrían acordar parámetros básicos (tomando prestados en parte los principios militares de IA que Estados Unidos y otros países han propuesto) para garantizar que el uso de armas de IA sea consistente con el derecho internacional humanitario y que existan salvaguardas para mitigar el riesgo de una escalada involuntaria y fallos catastróficos.</p><p>Esta no es la primera vez que los líderes internacionales se enfrentan al potencial devastador de las nuevas tecnologías. Así como los líderes mundiales alcanzaron un consenso después de la Segunda Guerra Mundial para crear barreras de comportamiento a través de los Convenios de Ginebra, los líderes internacionales deberían emprender un esfuerzo similar para las tecnologías de inteligencia artificial. Las democracias liberales pueden desempeñar un papel mucho mayor en el establecimiento de normas y condiciones básicas para el despliegue de estas nuevas y poderosas tecnologías de guerra.</p><p>(Extractado de The Atomic Scientists Bulletin)</p><p>​</p><p>​</p></div>",
  "author": "Redacción",
  "favicon": "https://www.elobservador.com.uy/images/favicons/favicon-16x16.png",
  "source": "elobservador.com.uy",
  "published": "2024-01-15T08:00:48.857Z",
  "ttr": 327,
  "type": "article"
}